---
layout: posts
prev: lab_meeting
title: "[IEEE â€˜20] Cell On/Off Parameter Optimization for Saving Energy via Reinforcement Learning"
date: 2024-05-10 00:41:09
name: ê¹€ê±´
URL: https://ieeexplore.ieee.org/document/9682160
file: /_lab_meeting/files/1.pdf
teaser: /_lab_meeting/teasers/1.png
keywords: [Cell on/off, Energy-saving, Network, Artificial Intelligence,]
---

> **ë°œì œì**: {{ page.name }}
>
> **ë°œì œì¼**: {{ page.date | date: "%Yë…„ %mì›” %dì¼" }}
>
> **í‚¤ì›Œë“œ**: {% for keyword in page.keywords %} [{{ keyword }}] {% endfor %}
>
> **URL**: [{{ page.URL }}]({{ page.URL }}){:target="_blank"}
>
> **ë°œí‘œìë£Œ**: {% if page.file %} [**PDF**]({{ page.file }}){:target="_blank"} {% endif %}


# Why this paper

- Energy-savingì„ ì‹¤í˜„í•˜ê³  QoS ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” cell ì˜¨ì˜¤í”„ ì•Œê³ ë¦¬ì¦˜ì„ RL ë¬¸ì œë¡œ ì •ì˜
- ë‹¤ì–‘í•œ Operational modeì—ì„œ trainingëœ RL agentë“¤ê°„ì˜ ì„±ëŠ¥ì„ ë¹„êµ
- í˜„ì‹¤ ì„¸ê³„ë¥¼ ëª¨ë°©í•œ replicatvie simulatorë¥¼ í™œìš©í•˜ì—¬ trainingí•œ ëª¨ë¸ ì‚¬ìš©

# Summary of paper

## 1. Introduction

- **ì—°êµ¬ ë°°ê²½**
    - ì‚¬ìš©ìë“¤ì´ ìš”êµ¬í•˜ëŠ” íŠ¸ë˜í”½ì–‘ì˜ ì¦ê°€ì— ë”°ë¼ base stationì„ ì¢ì€ ì§€ì—­ì— ì¡°ë°€í•˜ê²Œ ë°°ì¹˜í•˜ê³  ìˆìŒ
    - ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±ì—ì„œ Base stationì´ ì „ì²´ ì†Œëª¨ íŒŒì›Œì˜ 40%ë¥¼ ì°¨ì§€í•˜ê³  ìˆìŒ
    - Base station ìˆ˜ê°€ ë§ì•„ì§ì— ë”°ë¼ energy-savingì˜ í•„ìš”ì„±ì´ ì¦ê°€í•¨

- **ì—°êµ¬ í•„ìš”ì„±**
    - Base stationì´ ë°©ì¶œí•˜ëŠ” íƒ„ì†ŒëŸ‰ì€ í™˜ê²½ ë³´ì¡´ì„ ìœ„í•´ ê°ì†Œë˜ì–´ì•¼ í•¨
    - Energy savingì„ í†µí•´ OPEX(Operation Expenditure: ìš´ì˜ ë¹„ìš©)ì„ ì ˆê°í•  ìˆ˜ ìˆìŒ

- **ì—°êµ¬ ë¬¸ì œ**
    - ì‚¬ìš©ìë“¤ì˜ QoS(Quality of Service)ë¥¼ ë³´ì¥í•˜ë©´ì„œ Energy savingì„ ìµœëŒ€í™”
    - ê°•í™”í•™ìŠµ(RL: Reinforcement Learning)ì„ ì‚¬ìš©í•˜ì—¬

- **ì—°êµ¬ ìš”ì•½**
    - State: cellì˜ loadì™€ on/off ìƒíƒœ
    - Action: cellì˜ activation, deactivation threshold
    - Reward: power reward + throughput reward

- **Contribution**
    - Formulating an RL problem controlling a cell on/off algorithm in a way to minimize energy consumption while satisfying throughput constraints
    - Proposing a range of operational modes on top of the trained RL agent
    - Presenting experimental results with a replicative simulator

## 2. Related work

- Ye et al, HetNet êµ¬ì¡°ì—ì„œ ê°•í™”í•™ìŠµ ê¸°ë°˜ì˜ small cell on/off ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•¨. ì´ ë•Œ, Macro cellì˜ loadë¥¼ ê³ ì •ì‹œí‚¨ ì±„, ì‚¬ìš©ìë“¤ì˜ QoS ì œì•½ ì¡°ê±´ì„ ë§Œì¡±ì‹œí‚¤ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•¨
    - J. Ye and Y.-J. A. Zhang, â€œDrag: Deep reinforcement learning based base station activation in heterogeneous networks,â€ IEEE Transactions on Mobile Computing (TMC), 2019.
- Wu et al. ì˜ì—­ ì•ˆì— ìˆëŠ” ëª¨ë“  ì…€ë“¤ì˜ parameterë“¤ì˜ ì¡°ì •ì„ í†µí•´ power saving operationì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨
    - S. Wu, Y. Wang, and L. Bai, â€œDeep convolutional neural network assisted reinforcement learning based mobile network power saving,â€ IEEE Access, vol. 8, 2020.

## 3. System Model

### 3.1 Replicative Simulation

- ì‹¤ì œ ìƒìš© RAN í™˜ê²½ì—ì„œ ê°•í™”í•™ìŠµì„ ë°”ë¡œ ë„ì…í•˜ëŠ” ê²ƒì€ ì‚¬ìš©ìë“¤ì˜ experienceë¥¼ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°”ëŒì§í•˜ì§€ ì•ŠìŒ
- Replicative simulationì„ í™œìš©í•˜ì—¬ RL(Reinforcement Learning) ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨ì‹œí‚¨ í›„ ìƒìš© RAN í™˜ê²½ì— ë„ì…í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆ
- ì‹¤ì œ LTE í™˜ê²½ì— ë§ì¶”ì–´, 4ê°œì˜ spectrum bandsë¥¼ ê³ ë ¤í•œ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆ
    - Spectrum bandë§ˆë‹¤ ë‹¤ë¥¸ RUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ í™œìš©í•˜ì—¬, PRB utilizationì— ì°¨ì´ì ì„ ë¶€ì—¬í•¨

      <img class="modal img__small" src="/_lab_meeting/images/1/2.png" alt=""/>


### 3.2 Cell On/Off Algorithm

<div class="notice" markdown="1">
ğŸ’¡ ê° cellì€ `activation`, `deactivation`  ë‘ ê°œì˜ ë…ë¦½ì ì¸ threshold ê°’ì„ ê°€ì§
</div>

1. Cellì´ off ë˜ë©´, offëœ cellì— ìˆë˜ UEë“¤ì€ RSRPë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë¥¸ cellë“¤ë¡œ association ë¨
    1. UEë“¤ì´ distributeë˜ëŠ” ê²ƒì€ RSRPë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°„ë‹¨í•œ load balancing ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•¨
2. **ìµœì ì˜ threshold ê°’ë“¤**ì„ ì°¾ì•„ QoS ì œì•½ì¡°ê±´ì„ ë§Œì¡±ì‹œí‚¤ë©° ìµœëŒ€ energy savingì„ ì‹¤í˜„ì‹œí‚¬ ìˆ˜ ìˆìŒ

### 3.3 Power Model

- PowerëŠ” PRB(Physical Resource Block) utilizationì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ê²Œ ëª¨ë¸ë§
- PRB utilizationì´ 0ì´ë”ë¼ë„ Power ì†Œëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì€ ì•„ë‹˜

## 4. Reinforcement Learning-based Customized Energy Saving Policy

### 4.1 RL Model Design

<div class="notice" markdown="1">
ğŸ’¡ RL model: PPO (Proximal Policy Optimization)

- ìµœì ì˜ policy $\pi (a\|s)$ë¥¼ ë„ì¶œí•˜ê¸° ìœ„í•´ í•™ìŠµ
- Continuous, discrete ìƒí™©ì—ì„œë„ ëª¨ë‘ ê²¬ê³ í•œ í•™ìŠµ ëŠ¥ë ¥ì„ ë³´ì„
</div>


  <img class="modal img__small" src="/_lab_meeting/images/1/3.png" alt=""/>

- State

  $$
  s_t^i=(l_(t-1)^i, â€¦, l_(t-K)^i,
  c_(t-1)^i, ...
  , c_(t-K)^i)
  $$

  $ğ‘™_ğ‘¡^ğ‘–$Â : Load of cell ğ‘– at time ğ‘˜

  $ğ‘_ğ‘¡^ğ‘–$: On/Off state of cell ğ‘–

- Action
    - Cell ğ‘– at time ğ‘¡ì—ì„œì˜ activation, deactivation threshold ê°’ë“¤

- Reward
    - Power reward

        ğ‘Ÿ_ğ‘ğ‘œğ‘¤ğ‘’ğ‘Ÿ= $ğ›½_0^ğ‘+ ğ›½_1^ğ‘ ğ‘™_ğ‘¡^ğ‘–c$

        - $ğ›½_0^ğ‘: 1$
        - $Î²_1^p: -1/(P_max  - P_min)$

    - Throughput reward

        $r_{tput} =
        \begin{cases}
        Î²_0^tp+ Î²_1^{tp} k_t^i,\ \ \ \ \
        if \ \ k_t^i>Q \\
        Î²_0^d+ Î²_1^d k_t^i,\ \ \ \ \ \ \        otherwise
        \end{cases}$

        - ğ‘: Minimum throughput constraint
        - $ğ‘˜_ğ‘¡^ğ‘–$: Throughput of cell ğ‘– at time ğ‘¡
        - $ğ›½_0^{ğ‘¡ğ‘}$: âˆ’ğ‘/(ğ‘‡_ğ‘šğ‘ğ‘¥  âˆ’ğ‘)
        - $ğ›½_1^{ğ‘¡ğ‘}$: 1/(ğ‘‡_ğ‘šğ‘ğ‘¥âˆ’ğ‘)
        - $ğ›½_0^ğ‘‘$: âˆ’10ğ‘
        - $ğ›½_1^ğ‘‘$:10

    - Total reward = Power reward + Throughput reward


        $r_t= \sum(Î±* r_power+(1-Î±)*r_{tput} )$

        - where $\alpha$ is a real number satisfying $0 < \alpha < 1$

### 4.2 Various Operational Modes

- Controlê³¼ predictionì˜ periodë¥¼ ë³€í™”ì‹œí‚¤ë©´ì„œ í•™ìŠµ performanceì™€ required computation power ì‚¬ì´ì˜ ê· í˜•ì„ ë§ì¶œ ìˆ˜ ìˆìŒ

### 4.3 Comparative Evaluation

### 4.3.1 Simulation and System Settings

- ê°€ì¥ ë‚®ì€ spectrum bandsë¥¼ ì‚¬ìš©í•˜ëŠ” cellì€ í•­ìƒ ì¼œì ¸ìˆì–´ì•¼ í•¨ (Macro cellì€ í•­ìƒ ì¼œì ¸ìˆì–´ì•¼ í•¨, ì»¤ë²„ë¦¬ì§€ë¥¼ ë‹´ë‹¹í•˜ë¯€ë¡œ)
- 1ì¼ë™ì•ˆ activation, deactivation thresholdë¥¼ ìµœì í™”í•˜ëŠ” í•™ìŠµì„ ì§„í–‰í•¨

### 4.3.2 Evaluation Scenarios

- Traffic volumeì„ ë³€í™”ì‹œì¼œ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë“¤ì„ ìƒì„±

    <img class="modal img__small" src="/_lab_meeting/images/1/4.png" alt=""/>

- Cell On/Off Operating Optionsì€ ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ìŒ

    <img class="modal img__small" src="/_lab_meeting/images/1/5.png" alt=""/>


### 4.3.3 Evaluation Results

<img class="modal img__small" src="/_lab_meeting/images/1/6.png" alt="Throughput ì œí•œì´ 1Mbpsì¼ ë•Œ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ"/>

<img class="modal img__small" src="/_lab_meeting/images/1/7.png" alt="Throughput ì œí•œì´ 2Mbpsì¼ ë•Œ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ"/>

- RLì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì•Œê³ ë¦¬ì¦˜ë“¤ì´ Power savingì—ì„œ ì••ë„ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„
    - RL-hourlyê°€ ê°€ì¥ ì¢‹ì€ energy savingì„ ë³´ì„ì„ ì•Œ ìˆ˜ ìˆìŒ

- Throughput ì œì•½ì„ ë” ì—„ê²©í•˜ê²Œ ì„¤ì •í•œ í›„ í•™ìŠµ ì§„í–‰
    - ì˜ˆìƒê³¼ ë‹¤ë¥´ê²Œ constraintsì´ ë” ëŠìŠ¨í•  ë•Œ ë§ì€ ì—ë„ˆì§€ë¥¼ ì†Œëª¨í•˜ê²Œ ë¨

## 5. Conclusion

1. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ RL ì•Œê³ ë¦¬ì¦˜ì´ throughput ì œì•½ì¡°ê±´ì„ ë§Œì¡±ì‹œí‚¤ë©° ê°€ì¥ ë†’ì€ energy savingì„ ë‹¬ì„±í•¨
2. ë„¤íŠ¸ì›Œí¬ status predictionì„ ê°™ì´ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ base stationì˜ CAPEXë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŒ

# Take away

1. ì‹¤ì œ í™˜ê²½ì„ ë°˜ì˜í•˜ì—¬ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì„ ê³ ë ¤í•œ cell on/off ì•Œê³ ë¦¬ì¦˜
2. **ê°•í™”í•™ìŠµì„ ìœ„í•´ í•„ìš”í•œ íŒŒë¼ë¯¸í„°ë“¤ì„ ê¼­ ê°€ì ¸ê°ˆ ê²ƒ**
3. ê°•í™”í•™ìŠµì„ ì‹¤ì œ ìƒí™©ì— ë°”ë¡œ ë„ì…í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ replicative simulatorì—ì„œ í•™ìŠµ í›„ RANì— ë„ì…í•œë‹¤ëŠ” ê²ƒ ê¸°ì–µí•˜ê¸°